{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torchvision\n",
    "from torch import optim,nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# ae complexity\n",
    "img_size = 288\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((img_size,img_size)),\n",
    "    #T.RandomResizedCrop(image_size), # data augmentation\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "train_batch = 128\n",
    "test_batch = 32\n",
    "lr=1e-2\n",
    "data_root = \"/data/students/louis/standfordcars/standfordcars\"\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AE, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=kwargs[\"input_shape\"], out_features=256)\n",
    "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        # self.enc5 = nn.Linear(in_features=32, out_features=16)\n",
    "        # decoder \n",
    "        # self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dec5 = nn.Linear(in_features=256, out_features=kwargs[\"input_shape\"])\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        # x = F.relu(self.nc5(x))\n",
    "        # x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.relu(self.dec5(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "train_dataset = StanfordCars(root=data_root,split =\"test\",transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=train_batch, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(  test_dataset, test_batch, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class CAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "       \n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.in_latent = int(img_size/np.power(2,3))\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*3*self.in_latent*self.in_latent),\n",
    "            act_fn()\n",
    "        )\n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 288x288 => 144x144\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 144x144 => 72x72\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 72x72 => 36x36\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(in_features= 2*self.in_latent*self.in_latent*c_hid, out_features= latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 36x36 => 72x72\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 72x72 => 144x144\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 144x144 => 288x288\n",
    "            nn.Tanh() # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.linear(x)\n",
    "            x = x.reshape(x.shape[0], -1, self.in_latent, self.in_latent)\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\",0)\n",
    "print(device)\n",
    "\n",
    "# model = AE(input_shape=img_size*img_size*3).to(device)\n",
    "model = CAE(3,3,1000).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "restored_imgs = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        \n",
    "        # batch_features = batch_features.view(-1, img_size*img_size*3).to(device)\n",
    "        batch_features = batch_features.to(device)\n",
    "\n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    losses.append(loss)\n",
    "    restored_imgs.append((epochs, batch_features, outputs))\n",
    " \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f1ab2ca0b4268617c9c15108406e3be2b64989c728e47a18201667124c6f34f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
